{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed77513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "12ed2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "Non_MI_data = np.load('Non_MI_data.npy')\n",
    "Non_MI_label = np.load('Non_MI_label.npy')\n",
    "MI_generated_data = np.load('MI_generated_data.npy')\n",
    "MI_generated_label = np.load('MI_generated_label.npy')\n",
    "MI_data = np.load('MI_data.npy')\n",
    "MI_label = np.load('MI_label.npy')\n",
    "\n",
    "X_train_MI = np.load('MI_train_Kmeans.npy')\n",
    "X_test_MI = np.load('MI_test_Kmeans.npy')\n",
    "y_train_MI = np.load('MI_train_Kmeans_label.npy').tolist()\n",
    "y_test_MI = np.load('MI_test_Kmeans_label.npy').tolist()\n",
    "X_val_MI = np.load('MI_val_Kmeans.npy')\n",
    "y_val_MI = np.load('MI_val_Kmeans_label.npy').tolist()\n",
    "\n",
    "\n",
    "X_train_Non_MI = np.load('Non_MI_train_Kmeans.npy')\n",
    "X_test_Non_MI = np.load('Non_MI_test_Kmeans.npy')\n",
    "y_train_Non_MI = np.load('Non_MI_train_Kmeans_label.npy').tolist()\n",
    "y_test_Non_MI = np.load('Non_MI_test_Kmeans_label.npy').tolist()\n",
    "X_val_Non_MI = np.load('Non_MI_val_Kmeans.npy')\n",
    "y_val_Non_MI = np.load('Non_MI_val_Kmeans_label.npy').tolist()\n",
    "\n",
    "\n",
    "y_generatedMI = []\n",
    "y_train_MI =[]\n",
    "\n",
    "for i in MI_generated_data:\n",
    "    y_generatedMI.append(1) \n",
    "for i in X_train_MI:\n",
    "    y_train_MI.append(1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e08e1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first time select samples: [(0, 112), (1, 1058)]\n",
      "Second time balance samples: [(0, 1058), (1, 1057)]\n"
     ]
    }
   ],
   "source": [
    "#adding generated data and using twice SMOTE-ENN for sampling\n",
    "smote_enn = SMOTEENN(random_state = 4)\n",
    "from collections import Counter\n",
    "for j in range(95):\n",
    "    X_train = np.vstack((X_train_MI,X_train_Non_MI,MI_generated_data[:100+10*j]))\n",
    "    X_test = np.vstack((X_test_MI,X_test_Non_MI))\n",
    "    X_val = np.vstack((X_val_MI,X_val_Non_MI))\n",
    "\n",
    "\n",
    "    y_train = np.array(y_train_MI + y_train_Non_MI+ y_generatedMI[:100+10*j])\n",
    "    y_test = np.array(y_test_MI + y_test_Non_MI)\n",
    "    y_val = np.array(y_val_MI + y_val_Non_MI)\n",
    "#first time for selecting sample    \n",
    "    X_train,y_train = smote_enn.fit_resample(X_train,y_train)\n",
    "#each time need finetune these two parameters for selection and balance\n",
    "    if np.sum(y_train==1)>1000 and np.sum(y_train==0)>100:\n",
    "        print('first time select samples:',sorted(Counter(y_train).items()))\n",
    "        break\n",
    "#second time for balance\n",
    "X_train,y_train = smote_enn.fit_resample(X_train,y_train)   \n",
    "print('Second time balance samples:',sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "59fc9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "d:\\softwares\\python37\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "torch_x = torch.from_numpy(X_train)\n",
    "torch_y = torch.from_numpy(y_train)\n",
    "\n",
    "test_x = torch.from_numpy(X_test)\n",
    "test_y = torch.from_numpy(y_test)\n",
    "\n",
    "test_x_val = torch.from_numpy(X_val)\n",
    "test_y_val = torch.from_numpy(y_val)\n",
    "\n",
    "y_train = torch.tensor(torch_y, dtype=torch.long)\n",
    "y_test = torch.tensor(test_y,dtype = torch.long)\n",
    "x_test = torch.tensor(test_x , dtype=torch.float32)\n",
    "x_train = torch.tensor(torch_x, dtype=torch.float32)\n",
    "x_val = torch.tensor(test_x_val , dtype=torch.float32)\n",
    "y_val = torch.tensor(test_y_val,  dtype = torch.long)\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "torch_x_MI = torch.from_numpy(X_train_MI)\n",
    "x_train_MI = torch.tensor(torch_x_MI, dtype=torch.float32)\n",
    "torch_y_MI = torch.from_numpy(np.array(y_train_MI))\n",
    "y_train_MI = torch.tensor(torch_y_MI, dtype=torch.long)\n",
    "\n",
    "Non_MI_data = torch.from_numpy(X_test_Non_MI)\n",
    "MI_data = torch.from_numpy(X_test_MI)\n",
    "Non_MI_data = torch.tensor(Non_MI_data, dtype=torch.float32)\n",
    "MI_data = torch.tensor(MI_data, dtype=torch.float32)\n",
    "\n",
    "Non_MI_label = torch.from_numpy(np.array(y_test_Non_MI))\n",
    "MI_label = torch.from_numpy(np.array(y_test_MI))\n",
    "Non_MI_label = torch.tensor(Non_MI_data, dtype=torch.long)\n",
    "MI_label = torch.tensor(MI_label, dtype=torch.long)\n",
    "\n",
    "\n",
    "Non_MI_data_val = torch.from_numpy(X_val_Non_MI)\n",
    "MI_data_val = torch.from_numpy(X_val_MI)\n",
    "Non_MI_data_val = torch.tensor(Non_MI_data_val, dtype=torch.float32)\n",
    "MI_data_val = torch.tensor(MI_data_val, dtype=torch.float32)\n",
    "\n",
    "Non_MI_label_val = torch.from_numpy(np.array(y_val_Non_MI))\n",
    "MI_label_val = torch.from_numpy(np.array(y_val_MI))\n",
    "Non_MI_label_val = torch.tensor(Non_MI_data_val, dtype=torch.long)\n",
    "MI_label_val = torch.tensor(MI_label_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "91e735d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "EPOCH = 200\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 1\n",
    "INPUT_SIZE = 382\n",
    "LR = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7dc34159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_val = x_val.reshape(x_val.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_train = x_train.reshape(x_train.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_train_MI = x_train_MI.reshape(x_train_MI.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "\n",
    "Non_MI_data = Non_MI_data.reshape(Non_MI_data.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "MI_data = MI_data.reshape(MI_data.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "Non_MI_data_val = Non_MI_data_val.reshape(Non_MI_data_val.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "MI_data_val = MI_data_val.reshape(MI_data_val.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x_train,y_train )\n",
    "train_loader = Data.DataLoader(dataset= torch_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "52432cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model design\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc  = nn.Sequential(                     \n",
    "            nn.Linear(382, 383//2),   \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(383//2,383//4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(383//4,383//8),\n",
    "        )\n",
    "        self.out = nn.Linear(383//8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        output = self.out(x)\n",
    "        return output, x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85073fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d47e8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn =  DNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = torch.nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f549ea47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0 ,val_acc: 0.7498383968972204 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6425339366515838\n",
      "round: 1 ,val_acc: 0.7521008403361344 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6470588235294118\n",
      "round: 2 ,val_acc: 0.7521008403361344 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6470588235294118\n",
      "round: 3 ,val_acc: 0.77650290885585 ,val_MI: 0.9285714285714286 ,val_Non_MI: 0.6244343891402715\n",
      "round: 4 ,val_acc: 0.7340012928248223 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6108597285067874\n",
      "round: 5 ,val_acc: 0.7923400129282483 ,val_MI: 0.9285714285714286 ,val_Non_MI: 0.6561085972850679\n",
      "round: 6 ,val_acc: 0.7475759534583064 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6380090497737556\n",
      "round: 7 ,val_acc: 0.7430510665804784 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6289592760180995\n",
      "round: 8 ,val_acc: 0.7430510665804784 ,val_MI: 0.8571428571428571 ,val_Non_MI: 0.6289592760180995\n",
      "round: 9 ,val_acc: 0.7412734324499031 ,val_MI: 0.7857142857142857 ,val_Non_MI: 0.6968325791855203\n"
     ]
    }
   ],
   "source": [
    "#training model 10 times, and test model with validation dataset\n",
    "\n",
    "ACC_G_D = float(np.load('ACC_D_G.npy'))\n",
    "val_list=[]\n",
    "predict_list = []\n",
    "for i in range(0,10):\n",
    "    rnn =  DNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    Loss_CNN = []\n",
    "    Acc_train = []\n",
    "    Acc_test = []\n",
    "    Acc_val = []\n",
    "    \n",
    "    Acc_MI = []\n",
    "    Acc_Non_MI = []\n",
    "    Times = []\n",
    "    times = 0\n",
    "    STEP = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_MI = 0\n",
    "    best_Non_MI = 0\n",
    "\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (x, y) in enumerate(train_loader):   \n",
    "            b_x = Variable(x)\n",
    "            b_y = Variable(y)   \n",
    "            output = rnn(b_x)[0]\n",
    "            output_MI = rnn(x_train_MI)[0]\n",
    "           \n",
    "            loss = loss_func(output, b_y)+ACC_G_D*((b_y == 1).sum()/len(y_train))*loss_func(output_MI,y_train_MI)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                STEP.append(epoch)\n",
    "                train_output, last_layer = rnn(x_train)\n",
    "                pred_y = torch.max(train_output, 1)[1].data.squeeze()\n",
    "                train_accuracy = (pred_y == y_train).sum().item() / float(y_train.size(0))\n",
    " \n",
    "                MI_data_valout, last_layer = rnn(MI_data_val)\n",
    "                pred_MI_val = torch.max(MI_data_valout, 1)[1].data.squeeze()\n",
    "                pred_MI_val = (pred_MI_val.tolist().count(1))/float(MI_data_val.size(0))\n",
    "                \n",
    "                Non_MI_data_valout, last_layer = rnn(Non_MI_data_val)\n",
    "                pred_Non_MI_val = torch.max(Non_MI_data_valout, 1)[1].data.squeeze()\n",
    "                pred_Non_MI_val = (pred_Non_MI_val.tolist().count(0))/float(Non_MI_data_val.size(0))\n",
    "                \n",
    "                \n",
    "                test_acc =  (pred_MI_val + pred_Non_MI_val)/2\n",
    "\n",
    "                \n",
    "                Acc_train.append(train_accuracy)\n",
    "                Acc_test.append(test_acc)\n",
    "\n",
    "                \n",
    "                Acc_MI.append(pred_MI_val)\n",
    "                Acc_Non_MI.append(pred_Non_MI_val)\n",
    "                Loss_CNN.append(loss.data)\n",
    "                times += 1\n",
    "                Times.append(times)\n",
    "\n",
    "                if  min(pred_MI_val,pred_Non_MI_val)>= 0.6 and test_acc>best_acc:\n",
    "                    \n",
    "                    best_acc  = test_acc\n",
    "                    best_MI = pred_MI_val\n",
    "                    best_Non_MI = pred_Non_MI_val\n",
    "                    \n",
    "                    torch.save(rnn,'dnnL_best_{}'.format(i)+'.pkl')\n",
    "                    \n",
    "    if best_acc == 0:\n",
    "        best_acc  = test_acc\n",
    "        best_MI = pred_MI_val\n",
    "        best_Non_MI = pred_Non_MI_val\n",
    "\n",
    "        torch.save(rnn,'dnnL_best_{}'.format(i)+'.pkl')\n",
    "        \n",
    "    np.save('Acc_train_best_{}'.format(i)+'.npy',Acc_train)\n",
    "    np.save('Acc_val_best_{}'.format(i)+'.npy',Acc_val)\n",
    "    \n",
    "    np.save('Acc_testave_final_{}'.format(i)+'.npy',test_acc)\n",
    "    np.save('Acc_val_hf_{}'.format(i)+'.npy',pred_Non_MI_val)\n",
    "    np.save('Acc_val_sf_{}'.format(i)+'.npy',pred_MI_val)\n",
    "    \n",
    "    np.save('pred_Non_MI_dnnL_end_{}'.format(i)+'.npy',Acc_Non_MI)\n",
    "    np.save('Acc_train_dnnL_end_{}'.format(i)+'.npy',Acc_train)\n",
    "    np.save('Loss_dnnL_end_{}'.format(i)+'.npy',Loss_CNN)\n",
    "    np.save('STEP_dnnL_end_{}'.format(i)+'.npy',STEP)\n",
    "    np.save('Times_dnnL_end_{}'.format(i)+'.npy',Times)\n",
    "    val_list.append(best_acc)\n",
    "    print('round:',i,',val_acc:',best_acc,',val_MI:',best_MI,',val_Non_MI:',best_Non_MI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cfb1edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model with test dataset\n",
    "\n",
    "dnn_acc = []\n",
    "dnn_MI = []\n",
    "dnn_Non_MI = []\n",
    "\n",
    "for i in range(10):\n",
    "    rnn = torch.load('dnnL_best_{}'.format(i)+'.pkl')\n",
    "    \n",
    "    Non_MI_data_out, last_layer = rnn(Non_MI_data)\n",
    "    pred_Non_MI = torch.max(Non_MI_data_out, 1)[1].data.squeeze()\n",
    "    pred_Non_MI = (pred_Non_MI.tolist().count(0))/float(Non_MI_data.size(0))\n",
    "    MI_data_out, last_layer = rnn(MI_data)\n",
    "    pred_MI = torch.max(MI_data_out, 1)[1].data.squeeze()\n",
    "    pred_MI = (pred_MI.tolist().count(1))/float(MI_data.size(0))\n",
    "    predict_acc = (pred_Non_MI+pred_MI)/2 \n",
    "    \n",
    "    dnn_acc.append(predict_acc)\n",
    "    dnn_MI.append(pred_MI)\n",
    "    dnn_Non_MI.append(pred_Non_MI)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "88fd67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the average\n",
    "dnn_acc_mean = round(sum(dnn_acc)/10,2)\n",
    "dnn_MI_mean = round(sum(dnn_MI)/10,2)\n",
    "dnn_Non_MI_mean = round(sum(dnn_Non_MI)/10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e5e0c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy: 0.72\n",
      "final MI accuracy: 0.84\n",
      "final Non-MI accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('final accuracy:',dnn_acc_mean)\n",
    "print('final MI accuracy:',dnn_MI_mean)\n",
    "print('final Non-MI accuracy:',dnn_Non_MI_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is only one results of RS=32 from K-means, each time needs to finetune the SMOTE-ENN step, and after calculating, the average is 70%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
